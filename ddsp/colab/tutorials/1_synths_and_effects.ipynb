{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJd3dlOCStH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/tutorials/1_synths_and_effects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMqWDc_m6rUC"
      },
      "source": [
        "\n",
        "##### Copyright 2021 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNhgka4UKNjf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFIqwYGbZ-df"
      },
      "source": [
        "# DDSP Synths and Effects\n",
        "\n",
        "This notebook demonstrates the use of several of the Synths and Effects Processors in the DDSP library. While the core functions are also directly accessible through `ddsp.core`, using Processors is the preferred API for end-2-end training. \n",
        "\n",
        "As demonstrated in the [0_processors.ipynb](colab/tutorials/0_processors.ipynb) tutorial, Processors contain the necessary nonlinearities and preprocessing in their `get_controls()` method to convert generic neural network outputs into valid processor controls, which are then converted to signal by `get_signal()`. The two methods are called in series by `__call__()`.\n",
        "\n",
        "While each processor is capable of a wide range of expression, we focus on simple examples here for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jKDRJa6jztLT"
      },
      "outputs": [],
      "source": [
        "#@title Install DDSP\n",
        "\n",
        "#@markdown Install ddsp in a conda environment with Python 3.9 for compatibility.\n",
        "#@markdown This transfers a lot of data and _should take about 5 minutes_.\n",
        "#@markdown You can ignore warnings.\n",
        "\n",
        "!rm -rf /content/miniconda\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!/content/miniconda/bin/pip install tensorflow==2.11 tensorflow-probability==0.19.0 tensorflow-datasets==4.9.0 ddsp==3.7.0\n",
        "print('\\nDone installing DDSP in conda environment!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "helpers_setup"
      },
      "outputs": [],
      "source": [
        "#@title Import display helpers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal as scipy_signal\n",
        "\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output\n",
        "\n",
        "sample_rate = 16000\n",
        "\n",
        "\n",
        "def play(array_of_floats, sample_rate=sample_rate):\n",
        "  \"\"\"Play audio in colab using HTML5 audio widget.\"\"\"\n",
        "  if isinstance(array_of_floats, list):\n",
        "    array_of_floats = np.array(array_of_floats)\n",
        "  if len(array_of_floats.shape) == 2:\n",
        "    array_of_floats = array_of_floats[0]\n",
        "  normalizer = float(np.iinfo(np.int16).max)\n",
        "  array_of_ints = np.array(\n",
        "      np.asarray(array_of_floats) * normalizer, dtype=np.int16)\n",
        "  memfile = io.BytesIO()\n",
        "  wavfile.write(memfile, sample_rate, array_of_ints)\n",
        "  html = \"\"\"<audio controls>\n",
        "              <source controls src=\"data:audio/wav;base64,{base64_wavfile}\"\n",
        "              type=\"audio/wav\" />\n",
        "              Your browser does not support the audio element.\n",
        "            </audio>\"\"\"\n",
        "  html = html.format(\n",
        "      base64_wavfile=base64.b64encode(memfile.getvalue()).decode('ascii'))\n",
        "  memfile.close()\n",
        "  display.display(display.HTML(html))\n",
        "\n",
        "\n",
        "def specplot(audio, vmin=-5, vmax=1, rotate=True, size=512 + 256):\n",
        "  \"\"\"Plot the log magnitude spectrogram of audio.\"\"\"\n",
        "  if isinstance(audio, list):\n",
        "    audio = np.array(audio)\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = audio[0]\n",
        "  f, t, Sxx = scipy_signal.stft(audio, fs=sample_rate, nperseg=size,\n",
        "                                 noverlap=size * 3 // 4)\n",
        "  logmag = np.log10(np.abs(Sxx) + 1e-7)\n",
        "  if rotate:\n",
        "    logmag = np.flipud(logmag)\n",
        "  plt.matshow(logmag, vmin=vmin, vmax=vmax, cmap=plt.cm.magma, aspect='auto')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "def record_audio(seconds=3, sample_rate=sample_rate):\n",
        "  \"\"\"Record audio from the browser microphone.\"\"\"\n",
        "  record_js_code = \"\"\"\n",
        "  const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "  const b2text = blob => new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = e => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "  })\n",
        "\n",
        "  var record = time => new Promise(async resolve => {\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "    recorder = new MediaRecorder(stream)\n",
        "    chunks = []\n",
        "    recorder.ondataavailable = e => chunks.push(e.data)\n",
        "    recorder.start()\n",
        "    await sleep(time)\n",
        "    recorder.onstop = async ()=>{\n",
        "      blob = new Blob(chunks)\n",
        "      text = await b2text(blob)\n",
        "      resolve(text)\n",
        "    }\n",
        "    recorder.stop()\n",
        "  })\n",
        "  \"\"\"\n",
        "  print('Starting recording for {} seconds...'.format(seconds))\n",
        "  display.display(display.Javascript(record_js_code))\n",
        "  audio_string = output.eval_js('record(%d)' % (seconds * 1000.0))\n",
        "  print('Finished recording!')\n",
        "  audio_bytes = base64.b64decode(audio_string.split(',')[1])\n",
        "  from pydub import AudioSegment\n",
        "  segment = AudioSegment.from_file(io.BytesIO(audio_bytes))\n",
        "  segment = segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)\n",
        "  samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n",
        "  samples = samples / float(np.iinfo(np.int16).max)\n",
        "  return samples\n",
        "\n",
        "\n",
        "def upload_audio(sample_rate=sample_rate):\n",
        "  \"\"\"Upload audio files and return (filenames, audio_arrays).\"\"\"\n",
        "  from pydub import AudioSegment\n",
        "  audio_files = colab_files.upload()\n",
        "  fnames = list(audio_files.keys())\n",
        "  audios = []\n",
        "  for fname in fnames:\n",
        "    segment = AudioSegment.from_file(io.BytesIO(audio_files[fname]))\n",
        "    segment = segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)\n",
        "    samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n",
        "    samples = samples / float(np.iinfo(np.int16).max)\n",
        "    audios.append(samples)\n",
        "  return fnames, audios\n",
        "\n",
        "\n",
        "print('Helpers imported!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jXC-hm09dyl"
      },
      "source": [
        "# Synths\n",
        "\n",
        "Synthesizers, located in `ddsp.synths`, take network outputs and produce a signal (usually used as audio). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "256dCv-T9xHi"
      },
      "source": [
        "## Harmonic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HxKR0UTGpyn"
      },
      "source": [
        "The harmonic synthesizer models a sound as a linear combination of harmonic sinusoids. Amplitude envelopes are generated with 50% overlapping hann windows. The final audio is cropped to `n_samples`.\n",
        "\n",
        "Inputs:\n",
        "* `amplitudes`: Amplitude envelope of the synthesizer output.\n",
        "* `harmonic_distribution`: Normalized amplitudes of each harmonic.\n",
        "* `frequencies`: Frequency in Hz of base oscillator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "synths_script"
      },
      "outputs": [],
      "source": [
        "#@title Run all synth demos (Harmonic, FilteredNoise, Wavetable)\n",
        "\n",
        "SCRIPT = r'''\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import ddsp\n",
        "import tensorflow as tf\n",
        "\n",
        "output_dir = '/content/synth_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "sample_rate = 16000\n",
        "\n",
        "# ===========================================================================\n",
        "# Harmonic Synth\n",
        "# ===========================================================================\n",
        "print('--- Harmonic Synth ---')\n",
        "n_frames = 1000\n",
        "hop_size = 64\n",
        "n_samples_h = n_frames * hop_size\n",
        "\n",
        "# Amplitude [batch, n_frames, 1].\n",
        "amps = np.linspace(1.0, -3.0, n_frames)\n",
        "amps = amps[np.newaxis, :, np.newaxis]\n",
        "\n",
        "# Harmonic Distribution [batch, n_frames, n_harmonics].\n",
        "n_harmonics = 20\n",
        "harmonic_distribution = np.ones([n_frames, 1]) * np.linspace(1.0, -1.0, n_harmonics)[np.newaxis, :]\n",
        "harmonic_distribution = harmonic_distribution[np.newaxis, :, :]\n",
        "\n",
        "# Fundamental frequency in Hz [batch, n_frames, 1].\n",
        "f0_hz = 440.0 * np.ones([1, n_frames, 1])\n",
        "\n",
        "# Create synthesizer and generate audio.\n",
        "harmonic_synth = ddsp.synths.Harmonic(n_samples=n_samples_h,\n",
        "                                      scale_fn=ddsp.core.exp_sigmoid,\n",
        "                                      sample_rate=sample_rate)\n",
        "audio_harmonic = harmonic_synth(amps, harmonic_distribution, f0_hz)\n",
        "audio_h = audio_harmonic.numpy() if hasattr(audio_harmonic, 'numpy') else np.array(audio_harmonic)\n",
        "np.save(os.path.join(output_dir, 'audio_harmonic.npy'), audio_h)\n",
        "print('Harmonic audio saved.')  \n",
        "\n",
        "# ===========================================================================\n",
        "# FilteredNoise Synth\n",
        "# ===========================================================================\n",
        "print('--- FilteredNoise Synth ---')\n",
        "n_frames_fn = 250\n",
        "n_frequencies = 1000\n",
        "n_samples_fn = 64000\n",
        "\n",
        "# Bandpass filters, [n_batch, n_frames, n_frequencies].\n",
        "magnitudes = [tf.sin(tf.linspace(0.0, w, n_frequencies)) for w in np.linspace(8.0, 80.0, n_frames_fn)]\n",
        "magnitudes = 0.5 * tf.stack(magnitudes)**4.0\n",
        "magnitudes = magnitudes[tf.newaxis, :, :]\n",
        "\n",
        "# Create synthesizer and generate audio.\n",
        "filtered_noise_synth = ddsp.synths.FilteredNoise(n_samples=n_samples_fn, scale_fn=None)\n",
        "audio_fn = filtered_noise_synth(magnitudes)\n",
        "audio_fn_np = audio_fn.numpy() if hasattr(audio_fn, 'numpy') else np.array(audio_fn)\n",
        "np.save(os.path.join(output_dir, 'audio_filtered_noise.npy'), audio_fn_np)\n",
        "print('FilteredNoise audio saved.')\n",
        "\n",
        "# ===========================================================================\n",
        "# Wavetable Synth\n",
        "# ===========================================================================\n",
        "print('--- Wavetable Synth ---')\n",
        "n_samples_wt = 64000\n",
        "n_wavetable = 2048\n",
        "n_frames_wt = 100\n",
        "\n",
        "# Amplitude [batch, n_frames, 1].\n",
        "amps_wt = tf.linspace(0.5, 1e-3, n_frames_wt)[tf.newaxis, :, tf.newaxis]\n",
        "\n",
        "# Fundamental frequency in Hz [batch, n_frames, 1].\n",
        "f0_hz_wt = 110 * tf.linspace(1.5, 1, n_frames_wt)[tf.newaxis, :, tf.newaxis]\n",
        "\n",
        "# Wavetables [batch, n_frames, n_wavetable].\n",
        "wavetable_sin = tf.sin(tf.linspace(0.0, 2.0 * np.pi, n_wavetable))\n",
        "wavetable_sin = wavetable_sin[tf.newaxis, tf.newaxis, :]\n",
        "wavetable_square = tf.cast(wavetable_sin > 0.0, tf.float32) * 2.0 - 1.0\n",
        "wavetables = tf.concat([wavetable_square, wavetable_sin], axis=1)\n",
        "wavetables = ddsp.core.resample(wavetables, n_frames_wt)\n",
        "\n",
        "# Create synthesizer and generate audio.\n",
        "wavetable_synth = ddsp.synths.Wavetable(n_samples=n_samples_wt,\n",
        "                                        sample_rate=sample_rate,\n",
        "                                        scale_fn=None)\n",
        "audio_wt = wavetable_synth(amps_wt, wavetables, f0_hz_wt)\n",
        "audio_wt_np = audio_wt.numpy() if hasattr(audio_wt, 'numpy') else np.array(audio_wt)\n",
        "np.save(os.path.join(output_dir, 'audio_wavetable.npy'), audio_wt_np)\n",
        "print('Wavetable audio saved.')\n",
        "\n",
        "print('\\nDone! All synth outputs saved.')\n",
        "'''\n",
        "\n",
        "with open('/content/synth_demos.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "\n",
        "!unset PYTHONPATH PYTHONHOME && /content/miniconda/bin/python /content/synth_demos.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ-ZK2gI-df6"
      },
      "outputs": [],
      "source": [
        "# Harmonic Synth\n",
        "audio_harmonic = np.load('/content/synth_outputs/audio_harmonic.npy')\n",
        "play(audio_harmonic)\n",
        "specplot(audio_harmonic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HmOvGzs9zHB"
      },
      "source": [
        "## Filtered Noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o8otCCoCRwe"
      },
      "source": [
        "\n",
        "The filtered noise synthesizer is a subtractive synthesizer that shapes white noise with a series of time-varying filter banks. \n",
        "\n",
        "Inputs:\n",
        "* `magnitudes`: Amplitude envelope of each filter bank (linearly spaced from 0Hz to the Nyquist frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQSsBpCw_0f6"
      },
      "outputs": [],
      "source": [
        "# FilteredNoise Synth\n",
        "audio_fn = np.load('/content/synth_outputs/audio_filtered_noise.npy')\n",
        "play(audio_fn)\n",
        "specplot(audio_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-K69x2790ad"
      },
      "source": [
        "## Wavetable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRWOXvZ5Gsek"
      },
      "source": [
        "The wavetable synthesizer generates audio through interpolative lookup from small chunks of waveforms (wavetables) provided by the network. In principle, it is very similar to the `Harmonic` synth, but with a parameterization in the waveform domain and generation using linear interpolation vs. cumulative summation of sinusoid phases.\n",
        "\n",
        "Inputs:\n",
        "* `amplitudes`: Amplitude envelope of the synthesizer output.\n",
        "* `wavetables`: A series of wavetables that are interpolated to cover n_samples.\n",
        "* `frequencies`: Frequency in Hz of base oscillator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfYZ9P5yCjO8"
      },
      "outputs": [],
      "source": [
        "# Wavetable Synth - notice the aliasing artifacts from linear interpolation.\n",
        "audio_wt = np.load('/content/synth_outputs/audio_wavetable.npy')\n",
        "play(audio_wt)\n",
        "specplot(audio_wt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_lPZWTN92YJ"
      },
      "source": [
        "# Effects\n",
        "\n",
        "Effects, located in `ddsp.effects` are different in that they take network outputs to transform a given audio signal. Some effects, such as Reverb, optionally have trainable parameters of their own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lay_6Ldw93ZL"
      },
      "source": [
        "## Reverb\n",
        "\n",
        "There are several types of reverberation processors in ddsp.\n",
        "\n",
        "* Reverb\n",
        "* ExpDecayReverb\n",
        "* FilteredNoiseReverb\n",
        "\n",
        "Unlike other processors, reverbs also have the option to treat the impulse response as a 'trainable' variable, and not require it from network outputs. This is helpful for instance if the room environment is the same for the whole dataset. To make the reverb trainable, just pass the kwarg `trainable=True` to the constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tfNrv4MQXMW3"
      },
      "outputs": [],
      "source": [
        "#@markdown Record or Upload Audio\n",
        "\n",
        "record_or_upload = \"Upload (.mp3 or .wav)\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =   5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio_input = record_audio(seconds=record_seconds)\n",
        "else:\n",
        "  filenames, audios = upload_audio()\n",
        "  audio_input = audios[0]\n",
        "\n",
        "# Add batch dimension\n",
        "audio_input = audio_input[np.newaxis, :]\n",
        "\n",
        "# Save for effects scripts\n",
        "np.save('/content/audio_input.npy', audio_input)\n",
        "\n",
        "# Listen.\n",
        "specplot(audio_input)\n",
        "play(audio_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "effects_reverb_script"
      },
      "outputs": [],
      "source": [
        "#@title Run Reverb and FIRFilter effects demos\n",
        "\n",
        "SCRIPT = r'''\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import ddsp\n",
        "import tensorflow as tf\n",
        "\n",
        "output_dir = '/content/effects_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "sample_rate = 16000\n",
        "\n",
        "# Load input audio\n",
        "audio = np.load('/content/audio_input.npy')\n",
        "\n",
        "# ===========================================================================\n",
        "# ExpDecayReverb\n",
        "# ===========================================================================\n",
        "print('--- ExpDecayReverb ---')\n",
        "reverb = ddsp.effects.ExpDecayReverb(reverb_length=48000)\n",
        "gain = [[-2.0]]\n",
        "decay = [[2.0]]\n",
        "audio_reverb = reverb(audio, gain, decay)\n",
        "audio_reverb_np = audio_reverb.numpy() if hasattr(audio_reverb, 'numpy') else np.array(audio_reverb)\n",
        "np.save(os.path.join(output_dir, 'audio_exp_reverb.npy'), audio_reverb_np)\n",
        "\n",
        "# ===========================================================================\n",
        "# FilteredNoiseReverb\n",
        "# ===========================================================================\n",
        "print('--- FilteredNoiseReverb ---')\n",
        "reverb2 = ddsp.effects.FilteredNoiseReverb(reverb_length=48000, scale_fn=None)\n",
        "\n",
        "n_frames = 1000\n",
        "n_frequencies = 100\n",
        "frequencies = np.linspace(0, sample_rate / 2.0, n_frequencies)\n",
        "center_frequency = 4000.0 * np.linspace(0, 1.0, n_frames)\n",
        "width = 500.0\n",
        "gauss = lambda x, mu: 2.0 * np.pi * width**-2.0 * np.exp(- ((x - mu) / width)**2.0)\n",
        "magnitudes = np.array([gauss(frequencies, cf) for cf in center_frequency])\n",
        "magnitudes = magnitudes[np.newaxis, ...]\n",
        "magnitudes /= magnitudes.sum(axis=-1, keepdims=True) * 5\n",
        "\n",
        "audio_fn_reverb = reverb2(audio, magnitudes)\n",
        "audio_fn_reverb_np = audio_fn_reverb.numpy() if hasattr(audio_fn_reverb, 'numpy') else np.array(audio_fn_reverb)\n",
        "np.save(os.path.join(output_dir, 'audio_fn_reverb.npy'), audio_fn_reverb_np)\n",
        "np.save(os.path.join(output_dir, 'fn_reverb_magnitudes.npy'), magnitudes)\n",
        "\n",
        "# ===========================================================================\n",
        "# FIRFilter\n",
        "# ===========================================================================\n",
        "print('--- FIRFilter ---')\n",
        "fir_filter = ddsp.effects.FIRFilter(scale_fn=None)\n",
        "\n",
        "n_seconds = audio.size / sample_rate\n",
        "frame_rate = 100\n",
        "n_frames_fir = int(n_seconds * frame_rate)\n",
        "n_samples_fir = int(n_frames_fir * sample_rate / frame_rate)\n",
        "audio_trimmed = audio[:, :n_samples_fir]\n",
        "\n",
        "n_frequencies_fir = 1000\n",
        "frequencies_fir = np.linspace(0, sample_rate / 2.0, n_frequencies_fir)\n",
        "lfo_rate = 0.5\n",
        "n_cycles = n_seconds * lfo_rate\n",
        "center_frequency_fir = 1000 + 500 * np.sin(np.linspace(0, 2.0*np.pi*n_cycles, n_frames_fir))\n",
        "width_fir = 500.0\n",
        "gauss_fir = lambda x, mu: 2.0 * np.pi * width_fir**-2.0 * np.exp(- ((x - mu) / width_fir)**2.0)\n",
        "\n",
        "magnitudes_fir = np.array([gauss_fir(frequencies_fir, cf) for cf in center_frequency_fir])\n",
        "magnitudes_fir = magnitudes_fir[np.newaxis, ...]\n",
        "magnitudes_fir /= magnitudes_fir.max(axis=-1, keepdims=True)\n",
        "\n",
        "audio_fir = fir_filter(audio_trimmed, magnitudes_fir)\n",
        "audio_fir_np = audio_fir.numpy() if hasattr(audio_fir, 'numpy') else np.array(audio_fir)\n",
        "np.save(os.path.join(output_dir, 'audio_fir.npy'), audio_fir_np)\n",
        "np.save(os.path.join(output_dir, 'fir_magnitudes.npy'), magnitudes_fir)\n",
        "\n",
        "# ===========================================================================\n",
        "# ModDelay (Flanger, Chorus, Vibrato)\n",
        "# ===========================================================================\n",
        "print('--- ModDelay ---')\n",
        "\n",
        "def sin_phase(mod_rate, n_samples):\n",
        "    n_seconds = n_samples / sample_rate\n",
        "    phase = tf.sin(tf.linspace(0.0, mod_rate * n_seconds * 2.0 * np.pi, n_samples))\n",
        "    return phase[tf.newaxis, :, tf.newaxis]\n",
        "\n",
        "def modulate_audio(audio, center_ms, depth_ms, mod_rate, name):\n",
        "    mod_delay = ddsp.effects.ModDelay(center_ms=center_ms,\n",
        "                                      depth_ms=depth_ms,\n",
        "                                      gain_scale_fn=None,\n",
        "                                      phase_scale_fn=None)\n",
        "    phase = sin_phase(mod_rate, audio.size)\n",
        "    gain = 1.0 * np.ones_like(audio)[..., np.newaxis]\n",
        "    audio_out = 0.5 * mod_delay(audio, gain, phase)\n",
        "    out_np = audio_out.numpy() if hasattr(audio_out, 'numpy') else np.array(audio_out)\n",
        "    np.save(os.path.join(output_dir, f'audio_{name}.npy'), out_np)\n",
        "    print(f'{name} audio saved.')\n",
        "\n",
        "modulate_audio(audio, center_ms=0.75, depth_ms=0.75, mod_rate=0.25, name='flanger')\n",
        "modulate_audio(audio, center_ms=25.0, depth_ms=1.0, mod_rate=2.0, name='chorus')\n",
        "modulate_audio(audio, center_ms=25.0, depth_ms=12.5, mod_rate=5.0, name='vibrato')\n",
        "\n",
        "print('\\nDone! All effects outputs saved.')\n",
        "'''\n",
        "\n",
        "with open('/content/effects_demos.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "\n",
        "!unset PYTHONPATH PYTHONHOME && /content/miniconda/bin/python /content/effects_demos.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK951CM1XTGT"
      },
      "outputs": [],
      "source": [
        "# ExpDecayReverb\n",
        "audio_reverb = np.load('/content/effects_outputs/audio_exp_reverb.npy')\n",
        "print('ExpDecayReverb')\n",
        "specplot(audio_reverb)\n",
        "play(audio_reverb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skfxkn59VS01"
      },
      "outputs": [],
      "source": [
        "# FilteredNoiseReverb\n",
        "audio_fn_reverb = np.load('/content/effects_outputs/audio_fn_reverb.npy')\n",
        "fn_reverb_mags = np.load('/content/effects_outputs/fn_reverb_magnitudes.npy')\n",
        "\n",
        "print('FilteredNoiseReverb')\n",
        "specplot(audio_fn_reverb)\n",
        "play(audio_fn_reverb)\n",
        "plt.matshow(np.rot90(fn_reverb_mags[0]), aspect='auto')\n",
        "plt.title('Impulse Response Frequency Response')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks([])\n",
        "_ = plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc_md-cD99y6"
      },
      "source": [
        "## FIR Filter\n",
        "\n",
        "Linear time-varying finite impulse response (LTV-FIR) filters are a broad class of filters that can vary over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAJYBSUSfADI"
      },
      "outputs": [],
      "source": [
        "# FIRFilter\n",
        "audio_fir = np.load('/content/effects_outputs/audio_fir.npy')\n",
        "fir_mags = np.load('/content/effects_outputs/fir_magnitudes.npy')\n",
        "\n",
        "print('FIR Filter')\n",
        "play(audio_fir)\n",
        "specplot(audio_fir)\n",
        "_ = plt.matshow(np.rot90(fir_mags[0]), aspect='auto')\n",
        "plt.title('Frequency Response')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks([])\n",
        "_ = plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkjWUGpZ95Mr"
      },
      "source": [
        "## ModDelay\n",
        "\n",
        "Variable length delay lines create an instantaneous pitch shift that can be useful in a variety of time modulation effects such as [vibrato](https://en.wikipedia.org/wiki/Vibrato), [chorus](https://en.wikipedia.org/wiki/Chorus_effect), and [flanging](https://en.wikipedia.org/wiki/Flanging). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSEpt_DVEGbZ"
      },
      "outputs": [],
      "source": [
        "print('Flanger')\n",
        "audio_flanger = np.load('/content/effects_outputs/audio_flanger.npy')\n",
        "play(audio_flanger)\n",
        "specplot(audio_flanger)\n",
        "\n",
        "print('Chorus')\n",
        "audio_chorus = np.load('/content/effects_outputs/audio_chorus.npy')\n",
        "play(audio_chorus)\n",
        "specplot(audio_chorus)\n",
        "\n",
        "print('Vibrato')\n",
        "audio_vibrato = np.load('/content/effects_outputs/audio_vibrato.npy')\n",
        "play(audio_vibrato)\n",
        "specplot(audio_vibrato)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hMqWDc_m6rUC",
        "ZFIqwYGbZ-df",
        "6jXC-hm09dyl",
        "256dCv-T9xHi",
        "4HmOvGzs9zHB",
        "d-K69x2790ad",
        "C_lPZWTN92YJ",
        "Lay_6Ldw93ZL",
        "Lc_md-cD99y6",
        "mkjWUGpZ95Mr"
      ],
      "last_runtime": {},
      "name": "1_synths_and_effects.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}