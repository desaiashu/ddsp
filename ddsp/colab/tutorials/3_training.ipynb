{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJd3dlOCStH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/tutorials/3_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMqWDc_m6rUC"
      },
      "source": [
        "\n",
        "##### Copyright 2021 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNhgka4UKNjf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFIqwYGbZ-df"
      },
      "source": [
        "# DDSP Training\n",
        "\n",
        "This notebook demonstrates the libraries in [https://github.com/magenta/ddsp/tree/master/ddsp/training](https://github.com/magenta/ddsp/tree/master/ddsp/training). It is a simple example, overfitting a single audio sample, for educational purposes. \n",
        "\n",
        "_For a full training pipeline please use [ddsp/training/ddsp_run.py](https://github.com/magenta/ddsp/blob/main/ddsp/training/README.md#train-1) as in the [train_autoencoder.ipynb](https://github.com/magenta/ddsp/blob/main/ddsp/colab/demos/train_autoencoder.ipynb)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_jXCnwZ2QYW"
      },
      "outputs": [],
      "source": [
        "#@title Install DDSP\n",
        "\n",
        "#@markdown Install ddsp in a conda environment with Python 3.9 for compatibility.\n",
        "\n",
        "!rm -rf /content/miniconda\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!/content/miniconda/bin/pip install tensorflow==2.11 tensorflow-probability==0.19.0 tensorflow-datasets==4.9.0 ddsp==3.7.0\n",
        "print('\\nDone installing DDSP in conda environment!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "helpers_setup"
      },
      "outputs": [],
      "source": [
        "#@title Import display helpers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import base64\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal as scipy_signal\n",
        "\n",
        "sample_rate = 16000\n",
        "\n",
        "\n",
        "def play(array_of_floats, sample_rate=sample_rate):\n",
        "  \"\"\"Play audio in colab using HTML5 audio widget.\"\"\"\n",
        "  if isinstance(array_of_floats, list):\n",
        "    array_of_floats = np.array(array_of_floats)\n",
        "  if len(array_of_floats.shape) == 2:\n",
        "    array_of_floats = array_of_floats[0]\n",
        "  normalizer = float(np.iinfo(np.int16).max)\n",
        "  array_of_ints = np.array(\n",
        "      np.asarray(array_of_floats) * normalizer, dtype=np.int16)\n",
        "  memfile = io.BytesIO()\n",
        "  wavfile.write(memfile, sample_rate, array_of_ints)\n",
        "  html = \"\"\"<audio controls>\n",
        "              <source controls src=\"data:audio/wav;base64,{base64_wavfile}\"\n",
        "              type=\"audio/wav\" />\n",
        "              Your browser does not support the audio element.\n",
        "            </audio>\"\"\"\n",
        "  html = html.format(\n",
        "      base64_wavfile=base64.b64encode(memfile.getvalue()).decode('ascii'))\n",
        "  memfile.close()\n",
        "  display.display(display.HTML(html))\n",
        "\n",
        "\n",
        "def specplot(audio, vmin=-5, vmax=1, rotate=True, size=512 + 256):\n",
        "  \"\"\"Plot the log magnitude spectrogram of audio.\"\"\"\n",
        "  if isinstance(audio, list):\n",
        "    audio = np.array(audio)\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = audio[0]\n",
        "  f, t, Sxx = scipy_signal.stft(audio, fs=sample_rate, nperseg=size,\n",
        "                                 noverlap=size * 3 // 4)\n",
        "  logmag = np.log10(np.abs(Sxx) + 1e-7)\n",
        "  if rotate:\n",
        "    logmag = np.flipud(logmag)\n",
        "  plt.matshow(logmag, vmin=vmin, vmax=vmax, cmap=plt.cm.magma, aspect='auto')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "print('Helpers imported!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khYj8yiMDxGL"
      },
      "source": [
        "# Get a batch of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "data_script"
      },
      "outputs": [],
      "source": [
        "#@title Load NSynth data\n",
        "\n",
        "#@markdown This loads a single example from NSynth and saves it for use in training.\n",
        "\n",
        "SCRIPT = r'''\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "from ddsp.training import data\n",
        "\n",
        "output_dir = '/content/training_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get a single example from NSynth.\n",
        "data_provider = data.NSynthTfds(split='test')\n",
        "dataset = data_provider.get_batch(batch_size=1, shuffle=False).take(1).repeat()\n",
        "batch = next(iter(dataset))\n",
        "\n",
        "# Save audio for display\n",
        "audio = batch['audio'].numpy()\n",
        "np.save(os.path.join(output_dir, 'original_audio.npy'), audio)\n",
        "print('Audio shape:', audio.shape)\n",
        "print('Data loaded and saved.')\n",
        "'''\n",
        "\n",
        "with open('/content/training_data.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "\n",
        "!unset PYTHONPATH PYTHONHOME && /content/miniconda/bin/python /content/training_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzzaWKxVkYms"
      },
      "outputs": [],
      "source": [
        "audio = np.load('/content/training_outputs/original_audio.npy')\n",
        "specplot(audio)\n",
        "play(audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op0V8onI0VUK"
      },
      "source": [
        "# Get model and trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWZQXFLehCU0"
      },
      "source": [
        "## python "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnnxpYbRrPrp"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "python_training_script"
      },
      "outputs": [],
      "source": [
        "#@title Build model and train (Python config, 300 steps)\n",
        "\n",
        "#@markdown This builds the Autoencoder model using Python and trains for 300\n",
        "#@markdown steps on a single NSynth example. It saves:\n",
        "#@markdown - Original audio, resynthesized audio, noise audio\n",
        "#@markdown - Model parameters (amps, harmonic_distribution, noise_magnitudes, f0_hz, loudness)\n",
        "\n",
        "SCRIPT = r'''\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import ddsp\n",
        "from ddsp.training import (data, decoders, encoders, models, preprocessing,\n",
        "                           train_util, trainers)\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "output_dir = '/content/training_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "sample_rate = 16000\n",
        "\n",
        "# ===========================================================================\n",
        "# Load data\n",
        "# ===========================================================================\n",
        "print('Loading data...')\n",
        "data_provider = data.NSynthTfds(split='test')\n",
        "dataset = data_provider.get_batch(batch_size=1, shuffle=False).take(1).repeat()\n",
        "batch = next(iter(dataset))\n",
        "audio = batch['audio']\n",
        "n_samples = audio.shape[1]\n",
        "\n",
        "# ===========================================================================\n",
        "# Build model (Python config)\n",
        "# ===========================================================================\n",
        "print('Building model...')\n",
        "strategy = train_util.get_strategy()\n",
        "\n",
        "TIME_STEPS = 1000\n",
        "\n",
        "# Create Neural Networks.\n",
        "preprocessor = preprocessing.F0LoudnessPreprocessor(time_steps=TIME_STEPS)\n",
        "\n",
        "decoder = decoders.RnnFcDecoder(rnn_channels = 256,\n",
        "                                rnn_type = 'gru',\n",
        "                                ch = 256,\n",
        "                                layers_per_stack = 1,\n",
        "                                input_keys = ('ld_scaled', 'f0_scaled'),\n",
        "                                output_splits = (('amps', 1),\n",
        "                                                 ('harmonic_distribution', 45),\n",
        "                                                 ('noise_magnitudes', 45)))\n",
        "\n",
        "# Create Processors.\n",
        "harmonic = ddsp.synths.Harmonic(n_samples=n_samples,\n",
        "                                sample_rate=sample_rate,\n",
        "                                name='harmonic')\n",
        "\n",
        "noise = ddsp.synths.FilteredNoise(window_size=0,\n",
        "                                  initial_bias=-10.0,\n",
        "                                  name='noise')\n",
        "add = ddsp.processors.Add(name='add')\n",
        "\n",
        "# Create ProcessorGroup.\n",
        "dag = [(harmonic, ['amps', 'harmonic_distribution', 'f0_hz']),\n",
        "       (noise, ['noise_magnitudes']),\n",
        "       (add, ['noise/signal', 'harmonic/signal'])]\n",
        "\n",
        "processor_group = ddsp.processors.ProcessorGroup(dag=dag,\n",
        "                                                 name='processor_group')\n",
        "\n",
        "# Loss functions\n",
        "spectral_loss = ddsp.losses.SpectralLoss(loss_type='L1',\n",
        "                                         mag_weight=1.0,\n",
        "                                         logmag_weight=1.0)\n",
        "\n",
        "with strategy.scope():\n",
        "  model = models.Autoencoder(preprocessor=preprocessor,\n",
        "                             encoder=None,\n",
        "                             decoder=decoder,\n",
        "                             processor_group=processor_group,\n",
        "                             losses=[spectral_loss])\n",
        "  trainer = trainers.Trainer(model, strategy, learning_rate=1e-3)\n",
        "\n",
        "# ===========================================================================\n",
        "# Build and train\n",
        "# ===========================================================================\n",
        "print('Building model with first batch...')\n",
        "dataset_dist = trainer.distribute_dataset(dataset)\n",
        "trainer.build(next(iter(dataset_dist)))\n",
        "\n",
        "print('Training for 300 steps...')\n",
        "dataset_iter = iter(dataset_dist)\n",
        "for i in range(300):\n",
        "    losses = trainer.train_step(dataset_iter)\n",
        "    if i % 50 == 0 or i == 299:\n",
        "        res_str = 'step: {}\t'.format(i)\n",
        "        for k, v in losses.items():\n",
        "            res_str += '{}: {:.2f}\t'.format(k, v)\n",
        "        print(res_str)\n",
        "\n",
        "# ===========================================================================\n",
        "# Run inference and save results\n",
        "# ===========================================================================\n",
        "print('Running inference...')\n",
        "start_time = time.time()\n",
        "controls = model(next(dataset_iter))\n",
        "audio_gen = model.get_audio_from_outputs(controls)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Save original audio\n",
        "audio_np = audio.numpy()\n",
        "np.save(os.path.join(output_dir, 'original_audio.npy'), audio_np)\n",
        "\n",
        "# Save generated audio\n",
        "audio_gen_np = audio_gen.numpy() if hasattr(audio_gen, 'numpy') else np.array(audio_gen)\n",
        "np.save(os.path.join(output_dir, 'audio_gen.npy'), audio_gen_np)\n",
        "\n",
        "# Save noise signal\n",
        "audio_noise = controls['noise']['signal']\n",
        "audio_noise_np = audio_noise.numpy() if hasattr(audio_noise, 'numpy') else np.array(audio_noise)\n",
        "np.save(os.path.join(output_dir, 'audio_noise.npy'), audio_noise_np)\n",
        "\n",
        "# Save synth parameters for plotting\n",
        "batch_idx = 0\n",
        "get = lambda key: ddsp.core.nested_lookup(key, controls)[batch_idx]\n",
        "\n",
        "params = {\n",
        "    'amps': get('harmonic/controls/amplitudes'),\n",
        "    'harmonic_distribution': get('harmonic/controls/harmonic_distribution'),\n",
        "    'noise_magnitudes': get('noise/controls/magnitudes'),\n",
        "    'f0_hz': get('f0_hz'),\n",
        "    'loudness_db': get('loudness_db'),\n",
        "}\n",
        "for k, v in params.items():\n",
        "    v_np = v.numpy() if hasattr(v, 'numpy') else np.array(v)\n",
        "    np.save(os.path.join(output_dir, f'{k}.npy'), v_np)\n",
        "\n",
        "print('\\nDone! All training outputs saved.')\n",
        "'''\n",
        "\n",
        "with open('/content/training_run.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "\n",
        "!unset PYTHONPATH PYTHONHOME && /content/miniconda/bin/python /content/training_run.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cj220vSF8_Y"
      },
      "source": [
        "# Analyze results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlE5kkroHiFr"
      },
      "outputs": [],
      "source": [
        "audio = np.load('/content/training_outputs/original_audio.npy')\n",
        "audio_gen = np.load('/content/training_outputs/audio_gen.npy')\n",
        "audio_noise = np.load('/content/training_outputs/audio_noise.npy')\n",
        "\n",
        "print('Original Audio')\n",
        "play(audio)\n",
        "print('Resynthesized Audio')\n",
        "play(audio_gen)\n",
        "print('Filtered Noise Audio')\n",
        "play(audio_noise)\n",
        "\n",
        "specplot(audio)\n",
        "specplot(audio_gen)\n",
        "specplot(audio_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVhoLzV-ZYav"
      },
      "outputs": [],
      "source": [
        "amps = np.load('/content/training_outputs/amps.npy')\n",
        "harmonic_distribution = np.load('/content/training_outputs/harmonic_distribution.npy')\n",
        "noise_magnitudes = np.load('/content/training_outputs/noise_magnitudes.npy')\n",
        "f0_hz = np.load('/content/training_outputs/f0_hz.npy')\n",
        "loudness = np.load('/content/training_outputs/loudness_db.npy')\n",
        "\n",
        "f, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
        "f.suptitle('Input Features', fontsize=16)\n",
        "ax[0].plot(loudness)\n",
        "ax[0].set_ylabel('Loudness')\n",
        "ax[1].plot(f0_hz)\n",
        "ax[1].set_ylabel('F0_Hz')\n",
        "\n",
        "f, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
        "f.suptitle('Synth Params', fontsize=16)\n",
        "ax[0].semilogy(amps)\n",
        "ax[0].set_ylabel('Amps')\n",
        "ax[0].set_ylim(1e-5, 2)\n",
        "ax[1].matshow(np.rot90(np.log10(harmonic_distribution + 1e-6)),\n",
        "              cmap=plt.cm.magma, \n",
        "              aspect='auto')\n",
        "ax[1].set_ylabel('Harmonic Distribution')\n",
        "ax[1].set_xticks([])\n",
        "_ = ax[1].set_yticks([])\n",
        "\n",
        "f, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
        "ax.matshow(np.rot90(np.log10(noise_magnitudes + 1e-6)), \n",
        "           cmap=plt.cm.magma, \n",
        "           aspect='auto')\n",
        "ax.set_ylabel('Filtered Noise Magnitudes')\n",
        "ax.set_xticks([])\n",
        "_ = ax.set_yticks([])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZgDMV9hGyp"
      },
      "source": [
        "## Alternative: [`gin`](https://github.com/google/gin-config) configuration\n",
        "\n",
        "The model above can also be configured using gin. Run the cell below instead of the Python training cell above to see the same results with gin configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gin_training_script"
      },
      "outputs": [],
      "source": [
        "#@title Build model and train (Gin config, 300 steps)\n",
        "\n",
        "SCRIPT = r'''\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import ddsp\n",
        "from ddsp.training import (data, decoders, encoders, models, preprocessing,\n",
        "                           train_util, trainers)\n",
        "import gin\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "output_dir = '/content/training_outputs_gin'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "sample_rate = 16000\n",
        "\n",
        "# Load data\n",
        "print('Loading data...')\n",
        "data_provider = data.NSynthTfds(split='test')\n",
        "dataset = data_provider.get_batch(batch_size=1, shuffle=False).take(1).repeat()\n",
        "batch = next(iter(dataset))\n",
        "audio = batch['audio']\n",
        "n_samples = audio.shape[1]\n",
        "\n",
        "# Gin config\n",
        "gin_string = \"\"\"\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "\n",
        "# Preprocessor\n",
        "models.Autoencoder.preprocessor = @preprocessing.F0LoudnessPreprocessor()\n",
        "preprocessing.F0LoudnessPreprocessor.time_steps = 1000\n",
        "\n",
        "# Encoder\n",
        "models.Autoencoder.encoder = None\n",
        "\n",
        "# Decoder\n",
        "models.Autoencoder.decoder = @decoders.RnnFcDecoder()\n",
        "decoders.RnnFcDecoder.rnn_channels = 256\n",
        "decoders.RnnFcDecoder.rnn_type = 'gru'\n",
        "decoders.RnnFcDecoder.ch = 256\n",
        "decoders.RnnFcDecoder.layers_per_stack = 1\n",
        "decoders.RnnFcDecoder.input_keys = ('ld_scaled', 'f0_scaled')\n",
        "decoders.RnnFcDecoder.output_splits = (('amps', 1),\n",
        "                                       ('harmonic_distribution', 20),\n",
        "                                       ('noise_magnitudes', 20))\n",
        "\n",
        "# ProcessorGroup\n",
        "models.Autoencoder.processor_group = @processors.ProcessorGroup()\n",
        "\n",
        "processors.ProcessorGroup.dag = [\n",
        "  (@harmonic/synths.Harmonic(),\n",
        "    ['amps', 'harmonic_distribution', 'f0_hz']),\n",
        "  (@noise/synths.FilteredNoise(),\n",
        "    ['noise_magnitudes']),\n",
        "  (@add/processors.Add(),\n",
        "    ['noise/signal', 'harmonic/signal']),\n",
        "]\n",
        "\n",
        "# Harmonic Synthesizer\n",
        "harmonic/synths.Harmonic.name = 'harmonic'\n",
        "harmonic/synths.Harmonic.n_samples = 64000\n",
        "harmonic/synths.Harmonic.scale_fn = @core.exp_sigmoid\n",
        "\n",
        "# Filtered Noise Synthesizer\n",
        "noise/synths.FilteredNoise.name = 'noise'\n",
        "noise/synths.FilteredNoise.n_samples = 64000\n",
        "noise/synths.FilteredNoise.window_size = 0\n",
        "noise/synths.FilteredNoise.scale_fn = @core.exp_sigmoid\n",
        "noise/synths.FilteredNoise.initial_bias = -10.0\n",
        "\n",
        "# Add\n",
        "add/processors.Add.name = 'add'\n",
        "\n",
        "models.Autoencoder.losses = [\n",
        "    @losses.SpectralLoss(),\n",
        "]\n",
        "losses.SpectralLoss.loss_type = 'L1'\n",
        "losses.SpectralLoss.mag_weight = 1.0\n",
        "losses.SpectralLoss.logmag_weight = 1.0\n",
        "\"\"\"\n",
        "\n",
        "strategy = train_util.get_strategy()\n",
        "\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config(gin_string)\n",
        "\n",
        "with strategy.scope():\n",
        "    model = ddsp.training.models.Autoencoder()\n",
        "    trainer = trainers.Trainer(model, strategy, learning_rate=1e-4)\n",
        "\n",
        "# Build and train\n",
        "print('Building model...')\n",
        "dataset_dist = trainer.distribute_dataset(dataset)\n",
        "trainer.build(next(iter(dataset_dist)))\n",
        "\n",
        "print('Training for 300 steps...')\n",
        "dataset_iter = iter(dataset_dist)\n",
        "for i in range(300):\n",
        "    losses = trainer.train_step(dataset_iter)\n",
        "    if i % 50 == 0 or i == 299:\n",
        "        res_str = 'step: {}\t'.format(i)\n",
        "        for k, v in losses.items():\n",
        "            res_str += '{}: {:.2f}\t'.format(k, v)\n",
        "        print(res_str)\n",
        "\n",
        "# Inference\n",
        "print('Running inference...')\n",
        "controls = model(next(dataset_iter))\n",
        "audio_gen = model.get_audio_from_outputs(controls)\n",
        "\n",
        "# Save\n",
        "np.save(os.path.join(output_dir, 'original_audio.npy'), audio.numpy())\n",
        "audio_gen_np = audio_gen.numpy() if hasattr(audio_gen, 'numpy') else np.array(audio_gen)\n",
        "np.save(os.path.join(output_dir, 'audio_gen.npy'), audio_gen_np)\n",
        "\n",
        "audio_noise = controls['noise']['signal']\n",
        "audio_noise_np = audio_noise.numpy() if hasattr(audio_noise, 'numpy') else np.array(audio_noise)\n",
        "np.save(os.path.join(output_dir, 'audio_noise.npy'), audio_noise_np)\n",
        "\n",
        "print('\\nDone! Gin training outputs saved.')\n",
        "'''\n",
        "\n",
        "with open('/content/training_gin.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "\n",
        "!unset PYTHONPATH PYTHONHOME && /content/miniconda/bin/python /content/training_gin.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hMqWDc_m6rUC",
        "ZFIqwYGbZ-df",
        "khYj8yiMDxGL",
        "Op0V8onI0VUK",
        "EWZQXFLehCU0",
        "uAZgDMV9hGyp",
        "MnnxpYbRrPrp",
        "2cj220vSF8_Y"
      ],
      "last_runtime": {},
      "name": "3_training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}